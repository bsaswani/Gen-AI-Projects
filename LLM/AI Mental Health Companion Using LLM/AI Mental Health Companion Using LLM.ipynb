{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AI Mental Health Companion (LLM)**"
      ],
      "metadata": {
        "id": "iofIWYEfOCB2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "5-B_KV5WN5FX"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required libraries\n",
        "!pip install transformers datasets torch huggingface_hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "uYzrJGwfQhMs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Hugging Face authentication\n",
        "\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "if HF_TOKEN is None:\n",
        "    raise ValueError(\"Hugging Face token not found\")\n",
        "\n",
        "print(\"Token loaded!\")\n",
        "login(HF_TOKEN)"
      ],
      "metadata": {
        "id": "xWqaacvVhyhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7249e723-d138-4109-ef7f-66faf7ac6f92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load your CSV file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/mental_health.csv\")"
      ],
      "metadata": {
        "id": "vwkoT4aaSDZz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Preprocess CSV to split HUMAN and ASSISTANT\n",
        "df[['input_text', 'target_text']] = df['text'].str.extract(r'<HUMAN>:\\s*(.*?)\\s*<ASSISTANT>:\\s*(.*)')\n",
        "df = df.dropna()\n",
        "dataset = Dataset.from_pandas(df[['input_text', 'target_text']])\n",
        "print(\"Dataset:\", dataset)"
      ],
      "metadata": {
        "id": "9bYt8jBVQ4pJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff251f24-5966-4c5f-da60-3436fc0a18c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Dataset({\n",
            "    features: ['input_text', 'target_text'],\n",
            "    num_rows: 172\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load Llama-3.2-1B-Instruct\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "K5ZoGzYJRJwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e66de9-d5e6-444d-c681-99ff9836e394"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1010: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Create chat pipeline\n",
        "chat = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
      ],
      "metadata": {
        "id": "WWtHhOjcTNhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5121edf9-0936-478b-cad4-ad4dc1df664d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Define system prompt\n",
        "system_prompt = \"\"\"\n",
        "You are a warm, supportive mental health chatbot.\n",
        "Respond with ONE short, clear, and practical suggestion tailored to the user's concern.\n",
        "Avoid listing multiple options, repeating phrases, or using bullet points or numbers.\n",
        "Keep your tone empathetic, conversational, and calming.\n",
        "If the user shares progress, acknowledge it with encouragement.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RjLiANvf901u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Interactive LOOP\n",
        "import re\n",
        "\n",
        "print(\"ðŸ¤– Welcome to Mental Health Chatbot (type 'exit' to quit)\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"ðŸ¤– Bot: Take care! Goodbye ðŸ’™\")\n",
        "        break\n",
        "\n",
        "    prompt = f\"{system_prompt}\\nUser: {user_input}\\nAssistant:\"\n",
        "\n",
        "    response = chat(\n",
        "        prompt,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        truncation=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        return_full_text=False)\n",
        "\n",
        "    bot_reply = response[0]['generated_text']\n",
        "    bot_reply = bot_reply.replace(prompt, '').strip()\n",
        "    bot_reply = re.sub(r'(Assistant:)+', '', bot_reply).strip()\n",
        "\n",
        "    print(\"ðŸ¤– Bot:\", bot_reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0HMdQI9ulYh",
        "outputId": "fceeea0c-2811-4887-ca62-47b48a189eae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Welcome to Mental Health Chatbot (type 'exit' to quit)\n",
            "\n",
            "You: i am feeling stress today, please suggest one tip\n",
            "ðŸ¤– Bot: Sometimes, taking a few deep breaths can help calm the mind and body. Try inhaling for a count of 4, holding for 7, and exhaling for 8. You can do this anywhere, even just a minute or two. It might feel silly, but it can be really helpful in managing stress.\n",
            "You: yes i feel fresh and stress reduced\n",
            "ðŸ¤– Bot: I'm so glad to hear that! It sounds like you're on the right track. Keep up the good work and remember to take care of yourself. What's one thing you can do this week to show yourself some extra love and self-care?\n",
            "You: bye\n",
            "ðŸ¤– Bot: Take care! Goodbye ðŸ’™\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Test chat with a sample input\n",
        "sample_input = \"I am feeling stress today. What should I do?\"\n",
        "\n",
        "prompt = f\"{system_prompt}\\nUser: {sample_input}\\nAssistant:\"\n",
        "\n",
        "# Generate response\n",
        "response = chat(\n",
        "    prompt,\n",
        "    max_new_tokens=80,\n",
        "    do_sample=True,\n",
        "    truncation=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "bot_reply = response[0]['generated_text']\n",
        "bot_reply = re.sub(r'\\d+\\.\\s*', '', bot_reply)\n",
        "bot_reply = re.sub(r'[-*]\\s*', '', bot_reply)\n",
        "bot_reply = bot_reply.replace(prompt, '').strip()\n",
        "\n",
        "print(\"ðŸ¤– Bot response:\", bot_reply)\n"
      ],
      "metadata": {
        "id": "0vQTBZUWyMvc",
        "outputId": "253fa3da-747b-4d64-ceb0-e06d741145d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Bot response: Sometimes, taking a short walk outside can help clear your mind. Why don't you take 1015 minutes to step outside and breathe in some fresh air? You might find it helps calm you down.\n"
          ]
        }
      ]
    }
  ]
}